services:
  source_postgres:
    image: ${SOURCE_POSTGRES_IMAGE}
    ports:
      - "${SOURCE_POSTGRES_PORT}:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: ${SOURCE_POSTGRES_DB}
      POSTGRES_USER: ${SOURCE_POSTGRES_USER}
      POSTGRES_PASSWORD: ${SOURCE_POSTGRES_PASSWORD}
    volumes:
      - ${SOURCE_INIT_SQL}:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${SOURCE_POSTGRES_USER} -d ${SOURCE_POSTGRES_DB}"]
      interval: 2s
      timeout: 3s
      retries: 30

  destination_postgres:
    image: ${DEST_POSTGRES_IMAGE}
    ports:
      - "${DEST_POSTGRES_PORT}:5432"
    networks:
      - elt_network
    environment:
      POSTGRES_DB: ${DEST_POSTGRES_DB}
      POSTGRES_USER: ${DEST_POSTGRES_USER}
      POSTGRES_PASSWORD: ${DEST_POSTGRES_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DEST_POSTGRES_USER} -d ${DEST_POSTGRES_DB}"]
      interval: 2s
      timeout: 3s
      retries: 30

  # elt_script:
  #   build:
  #     context: ./elt
  #     dockerfile: Dockerfile
  #   command: ["python", "elt_script.py"]
  #   networks:
  #     - elt_network
  #   depends_on:
  #     source_postgres:
  #       condition: service_healthy
  #     destination_postgres:
  #       condition: service_healthy

  # dbt:
  #   image: ${DBT_IMAGE}
  #   command: ["run", "--profiles-dir", "/root", "--project-dir", "/dbt", "--profile", "custom_postgres", "--target", "dev"]
  #   networks:
  #     - elt_network
  #   volumes:
  #     - ${DBT_PROJECT_DIR}:/dbt
  #     - ${DBT_PROFILES_DIR}:/root/.dbt
  #   depends_on:
  #     destination_postgres:
  #       condition: service_healthy
  #     elt_script:
  #       condition: service_completed_successfully

  postgres:
    image: ${AIRFLOW_POSTGRES_IMAGE}
    networks:
      - elt_network
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB_NAME}

  # Common Airflow env (reused)
  # Note: compose will substitute ${...} from .env
  init-airflow:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - postgres
    networks:
      - elt_network
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_HOST}:${AIRFLOW_DB_PORT}/${AIRFLOW_DB_NAME}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: ${AIRFLOW_EXECUTION_API_SERVER_URL}
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      HOST_PROJECT_DIR: ${HOST_PROJECT_DIR}
      HOST_HOME: ${HOST_HOME}
    command: >
      bash -c "airflow db migrate"

  webserver:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${AIRFLOW_DAGS_DIR}:/opt/airflow/dags
      - ${AIRFLOW_ELT_DIR}:/opt/airflow/elt
      - ${DBT_PROJECT_DIR}:/opt/dbt
      - ${DBT_PROFILES_DIR}:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_HOST}:${AIRFLOW_DB_PORT}/${AIRFLOW_DB_NAME}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: ${AIRFLOW_EXECUTION_API_SERVER_URL}
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      HOST_PROJECT_DIR: ${HOST_PROJECT_DIR}
      HOST_HOME: ${HOST_HOME}
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    command: api-server

  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${AIRFLOW_DAGS_DIR}:/opt/airflow/dags
      - ${AIRFLOW_ELT_DIR}:/opt/airflow/elt
      - ${DBT_PROJECT_DIR}:/opt/dbt
      - ${DBT_PROFILES_DIR}:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_HOST}:${AIRFLOW_DB_PORT}/${AIRFLOW_DB_NAME}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: ${AIRFLOW_EXECUTION_API_SERVER_URL}
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      HOST_PROJECT_DIR: ${HOST_PROJECT_DIR}
      HOST_HOME: ${HOST_HOME}
    command: scheduler

  dag_processor:
    build:
      context: .
      dockerfile: Dockerfile
    user: root
    depends_on:
      - postgres
    networks:
      - elt_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${AIRFLOW_DAGS_DIR}:/opt/airflow/dags
      - ${AIRFLOW_ELT_DIR}:/opt/airflow/elt
      - ${DBT_PROJECT_DIR}:/opt/dbt
      - ${DBT_PROFILES_DIR}:/root/.dbt
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_HOST}:${AIRFLOW_DB_PORT}/${AIRFLOW_DB_NAME}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
      AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: ${AIRFLOW_EXECUTION_API_SERVER_URL}
      AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
      HOST_PROJECT_DIR: ${HOST_PROJECT_DIR}
      HOST_HOME: ${HOST_HOME}
    command: dag-processor

networks:
  elt_network:
    driver: bridge
